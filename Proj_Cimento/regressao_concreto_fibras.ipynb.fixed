{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": { "language": "markdown" },
   "source": ["# Regressão - Concreto com Fibras\n","Notebook reconstruído a partir de um arquivo .ipynb que continha apenas código Python; separei em células Markdown e Code para facilitar a leitura."]
  },
  { "cell_type": "code","execution_count": null,"metadata": {"language": "python"},"outputs": [],"source": ["# Importações e configurações iniciais\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('default')\nplt.rcParams['figure.figsize'] = (10, 6)\n"]},
  {"cell_type":"markdown","metadata":{"language":"markdown"},"source":["## Carregar / Definir os dados\nOs dados no arquivo original estavam embutidos como um dicionário Python. Aqui coloquei uma pequena amostra; substitua pela sua tabela completa ou carregue um CSV/Excel."]},
  {"cell_type":"code","execution_count": null,"metadata":{"language":"python"},"outputs": [],"source": ["# Dados de exemplo (substitua pelos seus dados completos ou carregue de um arquivo)\ndata = {\n    'Tipo_concreto': ['AM-0.4-EH1','AM-0.8-EH1','AM-0.4-EH2','AM-0.8-EH2'],\n    'fck_MPa': [61.3,63.8,63.6,58.7],\n    'l_mm': [35,35,60,60],\n    'd_mm': [0.55,0.55,0.9,0.9],\n    'l_d': [63.64,63.64,66.67,66.67],\n    'Teor_fibra_percent': [0.4,0.8,0.4,0.8],\n    'N_ganchos': [1,1,1,1],\n    'fR1_experimental': [4.99,7.44,5.35,7.94],\n    'fR3_experimental': [3.32,5.68,5.53,8.78]\n}\ndf = pd.DataFrame(data)\n# Alternativa: df = pd.read_excel('seu_arquivo.xlsx') ou pd.read_csv('seu_arquivo.csv')\n"]},
  {"cell_type":"code","execution_count": null,"metadata":{"language":"python"},"outputs": [],"source":["# Inspeção inicial\nprint('Primeiras linhas do DataFrame:')\ndisplay(df.head())\nprint(f'\nDimensões do dataset: {df.shape}')\nprint('\nTipos de dados:')\nprint(df.dtypes)\n"]},
  {"cell_type":"markdown","metadata":{"language":"markdown"},"source":["## Análise exploratória (gráficos)\nHistogramas e scatterplots para verificar distribuições e relações."]},
  {"cell_type":"code","execution_count": null,"metadata":{"language":"python"},"outputs": [],"source":["plt.figure(figsize=(15,10))\nplt.subplot(2,3,1)\nplt.hist(df['fck_MPa'], bins=20, alpha=0.7, color='blue')\nplt.title('Distribuição de fck (MPa)')\nplt.subplot(2,3,2)\nplt.hist(df['Teor_fibra_percent'], bins=20, alpha=0.7, color='green')\nplt.title('Distribuição do Teor de Fibra (%)')\nplt.subplot(2,3,3)\nplt.hist(df['fR1_experimental'], bins=20, alpha=0.7, color='red')\nplt.title('Distribuição de fR1 Experimental')\nplt.subplot(2,3,4)\nplt.scatter(df['fck_MPa'], df['fR1_experimental'], alpha=0.6)\nplt.title('fck vs fR1')\nplt.subplot(2,3,5)\nplt.scatter(df['Teor_fibra_percent'], df['fR1_experimental'], alpha=0.6)\nplt.title('Teor de Fibra vs fR1')\nplt.subplot(2,3,6)\nplt.scatter(df['l_d'], df['fR1_experimental'], alpha=0.6)\nplt.title('l/d vs fR1')\nplt.tight_layout()\nplt.show()\n"]},
  {"cell_type":"code","execution_count": null,"metadata":{"language":"python"},"outputs": [],"source":["# Matriz de correlação\ncorrelation_matrix = df[['fck_MPa','l_mm','d_mm','l_d','Teor_fibra_percent','N_ganchos','fR1_experimental','fR3_experimental']].corr()\nplt.figure(figsize=(10,8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, square=True, linewidths=0.5)\nplt.title('Matriz de Correlação entre Variáveis')\nplt.tight_layout()\nplt.show()\n"]},
  {"cell_type":"code","execution_count": null,"metadata":{"language":"python"},"outputs": [],"source":["# Preparação dos dados\nX = df[['fck_MPa','l_mm','d_mm','l_d','Teor_fibra_percent','N_ganchos']]\ny = df['fR1_experimental']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\nscaler_X = StandardScaler()\nscaler_y = StandardScaler()\nX_train_scaled = scaler_X.fit_transform(X_train)\nX_test_scaled = scaler_X.transform(X_test)\ny_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1,1))\ny_test_scaled = scaler_y.transform(y_test.values.reshape(-1,1))\nprint(f'X_train: {X_train.shape}, X_test: {X_test.shape}')\n"]},
  {"cell_type":"code","execution_count": null,"metadata":{"language":"python"},"outputs": [],"source":["def create_model(input_dim):\n    model = keras.Sequential([\n        layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n        layers.BatchNormalization(),\n        layers.Dropout(0.2),\n        layers.Dense(128, activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(64, activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.2),\n        layers.Dense(32, activation='relu'),\n        layers.Dropout(0.1),\n        layers.Dense(1, activation='linear')\n    ])\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n    return model\nmodel = create_model(X_train_scaled.shape[1])\nmodel.summary()\n"]},
  {"cell_type":"code","execution_count": null,"metadata":{"language":"python"},"outputs": [],"source":["early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\nreduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20, min_lr=1e-4)\nhistory = model.fit(X_train_scaled, y_train_scaled, epochs=500, batch_size=16, validation_data=(X_test_scaled, y_test_scaled), callbacks=[early_stopping, reduce_lr], verbose=1)\n"]},
  {"cell_type":"code","execution_count": null,"metadata":{"language":"python"},"outputs": [],"source":["# Avaliação\ny_pred_scaled = model.predict(X_test_scaled)\ny_pred = scaler_y.inverse_transform(y_pred_scaled)\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f'MSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}')\n"]},
  {"cell_type":"code","execution_count": null,"metadata":{"language":"python"},"outputs": [],"source":["# Importância por permutação e modelo fR3\nfrom sklearn.inspection import permutation_importance\nperm_importance = permutation_importance(model, X_test_scaled, y_test_scaled, n_repeats=10, random_state=42)\nfeature_importance = pd.DataFrame({'feature': X.columns, 'importance': perm_importance.importances_mean}).sort_values('importance', ascending=True)\nplt.figure(figsize=(10,6))\nplt.barh(feature_importance['feature'], feature_importance['importance'])\nplt.title('Importância das Variáveis (Permutação)')\nplt.tight_layout()\nplt.show()\n# Modelo para fR3 (mesma pipeline)\ny_fR3 = df['fR3_experimental']\nX_train_fR3, X_test_fR3, y_train_fR3, y_test_fR3 = train_test_split(X, y_fR3, test_size=0.2, random_state=42)\nX_train_fR3_scaled = scaler_X.transform(X_train_fR3)\nX_test_fR3_scaled = scaler_X.transform(X_test_fR3)\ny_train_fR3_scaled = scaler_y.fit_transform(y_train_fR3.values.reshape(-1,1))\ny_test_fR3_scaled = scaler_y.transform(y_test_fR3.values.reshape(-1,1))\nmodel_fR3 = create_model(X_train_fR3_scaled.shape[1])\nhistory_fR3 = model_fR3.fit(X_train_fR3_scaled, y_train_fR3_scaled, epochs=500, batch_size=16, validation_data=(X_test_fR3_scaled, y_test_fR3_scaled), callbacks=[early_stopping, reduce_lr], verbose=0)\ny_pred_fR3_scaled = model_fR3.predict(X_test_fR3_scaled)\ny_pred_fR3 = scaler_y.inverse_transform(y_pred_fR3_scaled)\nprint(f'R2 para fR3: {r2_score(y_test_fR3, y_pred_fR3):.4f}')\n"]},
  {"cell_type":"code","execution_count": null,"metadata":{"language":"python"},"outputs": [],"source":["# Funções utilitárias e salvar modelos\ndef predict_concrete_properties(model, scaler_X, scaler_y, input_data):\n    input_scaled = scaler_X.transform([input_data])\n    prediction_scaled = model.predict(input_scaled)\n    prediction = scaler_y.inverse_transform(prediction_scaled)\n    return float(prediction[0][0])\nnovo_concreto = [50.0,35.0,0.55,63.64,0.5,1.0]\npredicao_fR1 = predict_concrete_properties(model, scaler_X, scaler_y, novo_concreto)\npredicao_fR3 = predict_concrete_properties(model_fR3, scaler_X, scaler_y, novo_concreto)\nprint(f'fR1 previsto: {predicao_fR1:.2f} N/mm²')\nprint(f'fR3 previsto: {predicao_fR3:.2f} N/mm²')\nmodel.save('modelo_fR1.h5')\nmodel_fR3.save('modelo_fR3.h5')\nimport joblib\njoblib.dump(scaler_X, 'scaler_X.pkl')\njoblib.dump(scaler_y, 'scaler_y.pkl')\nprint('Modelos e scalers salvos com sucesso!')\n"]}
